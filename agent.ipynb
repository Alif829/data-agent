{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from playwright.sync_api import sync_playwright\n",
    "from serpapi import GoogleSearch\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "import re\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize summarization pipeline\n",
    "summarizer = pipeline('summarization', model=\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_pages(city: str) -> List[str]:\n",
    "    query = f\"{city} homicide statistics site:.gov OR site:.edu OR site:.org\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": os.getenv('SERPAPI_API_KEY'),\n",
    "        \"engine\": \"google\"\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    return [result['link'] for result in results.get('organic_results', [])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url: str) -> str:\n",
    "    parsed_url = urlparse(url)\n",
    "    if parsed_url.path.endswith('.pdf'):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with BytesIO(response.content) as open_pdf_file:\n",
    "                reader = PdfReader(open_pdf_file)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() if page.extract_text() else \"\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF {url}: {e}\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        try:\n",
    "            with sync_playwright() as p:\n",
    "                browser = p.chromium.launch(headless=True)\n",
    "                page = browser.new_page()\n",
    "                page.goto(url, timeout=60000)\n",
    "                content = page.content()\n",
    "                browser.close()\n",
    "\n",
    "                # Use BeautifulSoup to remove HTML tags\n",
    "                soup = BeautifulSoup(content, \"html.parser\")\n",
    "                text = soup.get_text()\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_content(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess content to remove irrelevant text and extract important sections.\n",
    "\n",
    "    Parameters:\n",
    "    content (str): The raw scraped content.\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed, cleaned content.\n",
    "    \"\"\"\n",
    "    # Remove irrelevant sections such as headers, footers, legal disclaimers, etc.\n",
    "    # Using a simple keyword-based filter to eliminate common irrelevant text\n",
    "    irrelevant_keywords = [\"privacy policy\", \"terms of service\", \"subscribe\", \"cookie policy\", \"advertisement\"]\n",
    "    lines = content.split('\\n')\n",
    "    relevant_lines = [\n",
    "        line for line in lines \n",
    "        if all(keyword.lower() not in line.lower() for keyword in irrelevant_keywords)\n",
    "    ]\n",
    "\n",
    "    # Join filtered lines and apply additional regex for year-based filtering\n",
    "    cleaned_content = \"\\n\".join(relevant_lines)\n",
    "\n",
    "    # Use regex to extract sentences containing years (targeting the last 5 years)\n",
    "    year_pattern = re.compile(r\"\\b(20[1-2][0-9])\\b\")  # Matches years 2010-2029\n",
    "    extracted_sentences = []\n",
    "    for sentence in cleaned_content.split('.'):\n",
    "        if year_pattern.search(sentence):\n",
    "            extracted_sentences.append(sentence.strip())\n",
    "\n",
    "    return \". \".join(extracted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_content(content: str, max_length: int = 150, chunk_size: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the content to reduce the length before sending it to the LLM.\n",
    "\n",
    "    Parameters:\n",
    "    content (str): The content to be summarized.\n",
    "    max_length (int): The maximum length of the summary.\n",
    "    chunk_size (int): The maximum length of each chunk for summarization.\n",
    "\n",
    "    Returns:\n",
    "    str: The summarized content.\n",
    "    \"\"\"\n",
    "    # Split content into smaller chunks to prevent exceeding the model's token limit\n",
    "    sentences = content.split('.')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        if len(' '.join(current_chunk).split()) > chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    # Summarize each chunk and concatenate the summaries\n",
    "    summarized_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            summary = summarizer(chunk, max_length=max_length, min_length=30, do_sample=False)\n",
    "            summarized_text += summary[0]['summary_text'] + \" \"\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing content: {e}\")\n",
    "            continue\n",
    "\n",
    "    return summarized_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_llm(content: str, city: str, num_years: int = 5) -> List[dict]:\n",
    "    # Preprocess the content to reduce token count\n",
    "    preprocessed_content = preprocess_content(content)\n",
    "\n",
    "    # Summarize the content to further reduce token usage\n",
    "    summarized_content = summarize_content(preprocessed_content)\n",
    "\n",
    "    # Split the summarized content into manageable chunks\n",
    "    chunks = split_content(summarized_content)\n",
    "    all_data = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = (\n",
    "            f\"Below is some information about homicide statistics for the city of {city}. \"\n",
    "            f\"Please extract the homicide counts for each year for the past {num_years} years. \"\n",
    "            f\"Provide the data in the format: Year, Homicide Count.\\n\\n\"\n",
    "            f\"Content:\\n{chunk}\"\n",
    "        )\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=300\n",
    "        )\n",
    "\n",
    "        response_text = response['choices'][0]['message']['content'].strip()\n",
    "        lines = response_text.split('\\n')\n",
    "        for line in lines:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) == 2:\n",
    "                try:\n",
    "                    year = int(parts[0].strip())\n",
    "                    count = int(parts[1].strip())\n",
    "                    all_data.append({\"Year\": year, \"Homicide Count\": count})\n",
    "                except ValueError:\n",
    "                    # Log the line that caused an error and continue\n",
    "                    print(f\"Skipping line due to ValueError: {line}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Skipping line due to incorrect format: {line}\")\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_content(content: str, max_chunk_size: int = 4000) -> List[str]:\n",
    "    words = content.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk)) > max_chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_homicide_data(cities: List[str], num_years: int = 5) -> pd.DataFrame:\n",
    "    all_data = []\n",
    "    for city in cities:\n",
    "        urls = search_relevant_pages(city)\n",
    "        city_data = []\n",
    "        for url in urls:\n",
    "            page_content = scrape_page(url)\n",
    "            if page_content:\n",
    "                structured_data = process_with_llm(page_content, city, num_years)\n",
    "                if structured_data:\n",
    "                    city_data.extend(structured_data)\n",
    "                    break  # Stop after finding the first valid page\n",
    "\n",
    "        if city_data:\n",
    "            for record in city_data:\n",
    "                all_data.append({\"City\": city, **record})\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://www.nyc.gov/site/nypd/news/p0527/nypd-citywide-crime-statistics-august-2024: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://en.wikipedia.org/wiki/Crime_in_New_York_City: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Skipping line due to ValueError: 2021, not provided\n",
      "Skipping line due to ValueError: 2022, not provided\n",
      "Error scraping https://usafacts.org/articles/which-cities-have-the-highest-murder-rates/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://crimebulletin.metrocrime.org/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://en.wikipedia.org/wiki/Crime_in_Louisiana: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://nola.gov/nopd/data/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://crimebulletin.metrocrime.org/orleans-crime-trends-as-of-january-28-2019-2024/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://cjrc.osu.edu/research/interdisciplinary/hvd/united-states/new-orleans: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://pubmed.ncbi.nlm.nih.gov/3189286/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://mayor.lacity.gov/news/lapd-releases-end-year-crime-statistics-city-los-angeles-2023: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://en.wikipedia.org/wiki/Crime_in_Los_Angeles: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "Error scraping https://www.ppic.org/publication/crime-trends-in-california/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Year</th>\n",
       "      <th>Homicide Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York</td>\n",
       "      <td>2019</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York</td>\n",
       "      <td>2020</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New York</td>\n",
       "      <td>2023</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Orleans</td>\n",
       "      <td>2017</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Orleans</td>\n",
       "      <td>2018</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Orleans</td>\n",
       "      <td>2019</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New Orleans</td>\n",
       "      <td>2020</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New Orleans</td>\n",
       "      <td>2021</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2019</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2020</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2021</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2022</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>2023</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           City  Year  Homicide Count\n",
       "0      New York  2019             649\n",
       "1      New York  2020             391\n",
       "2      New York  2023             629\n",
       "3   New Orleans  2017             157\n",
       "4   New Orleans  2018             146\n",
       "5   New Orleans  2019             120\n",
       "6   New Orleans  2020             200\n",
       "7   New Orleans  2021             180\n",
       "8   Los Angeles  2019             195\n",
       "9   Los Angeles  2020             281\n",
       "10  Los Angeles  2021             195\n",
       "11  Los Angeles  2022             259\n",
       "12  Los Angeles  2023             212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    cities = [\"New York\", \"New Orleans\", \"Los Angeles\"]\n",
    "    num_years = 5\n",
    "    homicide_data = collect_homicide_data(cities, num_years)\n",
    "    if not homicide_data.empty:\n",
    "        display(homicide_data)\n",
    "        homicide_data.to_csv(\"homicide_statistics_2.csv\", index=False)\n",
    "    else:\n",
    "        print(\"No data available to save.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
